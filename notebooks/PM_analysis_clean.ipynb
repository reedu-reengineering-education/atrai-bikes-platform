{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b19b69-a403-4bf9-87a0-4f54c5b9dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "initial_data = pd.read_csv ('combined_data_21_01_2025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae740456-afab-4042-9142-dbb4de331aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'device_id' and count the number of rows for each device\n",
    "device_counts = initial_data.groupby('device_id').size()\n",
    "\n",
    "# Filter out device_ids with fewer than 10 data entries\n",
    "valid_device_ids = device_counts[device_counts >= 10].index\n",
    "\n",
    "# Filter the original DataFrame to keep only rows with valid device_ids\n",
    "initial_data = initial_data[initial_data['device_id'].isin(valid_device_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5616141-7301-4d3f-b2f7-2d84f7217ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "from shapely import wkt\n",
    "\n",
    "# Function to get coordinates of the building address\n",
    "def get_coordinates(address):\n",
    "    geolocator = Nominatim(user_agent=\"Buffer_Creation\")\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        raise ValueError(f\"Address '{address}' not found.\")\n",
    "\n",
    "# Function to filter out rows within the buffer\n",
    "def filter_within_buffer(initial_data_w, address, radius_m):\n",
    "    # Get the building's coordinates (latitude, longitude)\n",
    "    building_lat, building_lon = get_coordinates(address)\n",
    "    \n",
    "    # Create a Point (building location)\n",
    "    building_point = Point(building_lon, building_lat)\n",
    "    \n",
    "    # Set up the projection for UTM (meter-based projection)\n",
    "    proj_wgs84 = pyproj.CRS('EPSG:4326')  # WGS84 (lat/lon)\n",
    "    proj_utm = pyproj.CRS('EPSG:32632')  # UTM zone 32N (adjust for your location)\n",
    "\n",
    "    # Transform the building point to UTM (to get meter-based coordinates)\n",
    "    transformer = pyproj.Transformer.from_crs(proj_wgs84, proj_utm, always_xy=True)\n",
    "    building_point_utm = transform(transformer.transform, building_point)\n",
    "\n",
    "    # Create a buffer in meters (UTM system uses meters)\n",
    "    building_buffer = building_point_utm.buffer(radius_m)\n",
    "\n",
    "    # Ensure the geometry column is in the correct format (Shapely geometries)\n",
    "    def safe_wkt_load(x):\n",
    "        if isinstance(x, str):  # Only try to load WKT strings\n",
    "            try:\n",
    "                return wkt.loads(x)\n",
    "            except:\n",
    "                return None  # Return None if the WKT is invalid\n",
    "        return None  # Return None for non-string entries\n",
    "\n",
    "    # Apply the safe WKT loading function\n",
    "    initial_data_w['geometry'] = initial_data_w['geometry'].apply(safe_wkt_load)\n",
    "\n",
    "    # Reproject the geometry column to UTM\n",
    "    initial_data_w['geometry_utm'] = initial_data_w['geometry'].apply(lambda point: transform(transformer.transform, point) if point is not None else None)\n",
    "    \n",
    "    # Filter rows where the UTM geometry is outside the buffer\n",
    "    filtered_data = initial_data_w[~initial_data_w['geometry_utm'].apply(lambda point: building_buffer.contains(point) if point is not None else False)]\n",
    "\n",
    "    # Drop the 'geometry_utm' column as it's no longer needed\n",
    "    filtered_data = filtered_data.drop(columns=['geometry_utm'])\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Example usage\n",
    "address = \"Von-Steuben-Straße 21, 48143 Münster\"\n",
    "initial_data_w = initial_data.copy(deep=True)  # Explicitly create a deep copy of the DataFrame\n",
    "\n",
    "# Apply the filter to get bike data within the buffer\n",
    "atrai_bike_data = filter_within_buffer(initial_data_w, address, radius_m=15)\n",
    "\n",
    "# View the filtered data\n",
    "#print(atrai_bike_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5684a3a7-f5de-407d-b89b-343d0f9fea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "atrai_bike_data_PM = atrai_bike_data[['createdAt', 'Rel. Humidity', 'Finedust PM1', 'Finedust PM2.5', 'Finedust PM4', 'Finedust PM10', 'geometry', 'device_id', 'lng', 'lat']] #filter for relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135ba45d-172b-46d2-bfec-210044e0fdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sauer\\AppData\\Local\\Temp\\ipykernel_29084\\3458098150.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  PM_data_no_outliers = PM_data_no_outliers.groupby('device_id', group_keys=False).apply(calculate_and_replace_outliers)\n",
      "C:\\Users\\sauer\\AppData\\Local\\Temp\\ipykernel_29084\\3458098150.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  PM_data_no_outliers = PM_data_no_outliers.groupby('device_id', group_keys=False).apply(calculate_and_replace_outliers)\n",
      "C:\\Users\\sauer\\AppData\\Local\\Temp\\ipykernel_29084\\3458098150.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  PM_data_no_outliers = PM_data_no_outliers.groupby('device_id', group_keys=False).apply(calculate_and_replace_outliers)\n",
      "C:\\Users\\sauer\\AppData\\Local\\Temp\\ipykernel_29084\\3458098150.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  PM_data_no_outliers = PM_data_no_outliers.groupby('device_id', group_keys=False).apply(calculate_and_replace_outliers)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filter the data based on Relative Humidity\n",
    "PM_data_filtered = atrai_bike_data_PM[(atrai_bike_data_PM['Rel. Humidity'] <= 75) & (atrai_bike_data_PM['Rel. Humidity'].notna())]\n",
    "\n",
    "def replace_outliers_with_nan_by_device(PM_data_no_outliers, column):\n",
    "    # Group by 'device_id' (sensebox) and apply the IQR calculation to each group\n",
    "    def calculate_and_replace_outliers(group):\n",
    "        # Calculate Q1, Q3, and IQR for the current column (outlier detection)\n",
    "        Q1 = group[column].quantile(0.25)\n",
    "        Q3 = group[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Replace values outside the bounds with NaN\n",
    "        group[column] = group[column].apply(lambda x: x if lower_bound <= x <= upper_bound else np.nan)\n",
    "        return group\n",
    "\n",
    "    # Apply the outlier replacement to each group (grouped by 'device_id')\n",
    "    PM_data_no_outliers = PM_data_no_outliers.groupby('device_id', group_keys=False).apply(calculate_and_replace_outliers)\n",
    "    \n",
    "    return PM_data_no_outliers\n",
    "\n",
    "# List of columns for PM values to check for outliers\n",
    "pm_columns = ['Finedust PM1', 'Finedust PM2.5', 'Finedust PM4', 'Finedust PM10']\n",
    "\n",
    "# Create a copy of the filtered data to avoid modifying the original\n",
    "PM_data_no_outliers = PM_data_filtered.copy(deep=True)\n",
    "\n",
    "# Loop through each column and replace outliers for each PM column\n",
    "for column in pm_columns:\n",
    "    PM_data_no_outliers = replace_outliers_with_nan_by_device(PM_data_no_outliers, column)\n",
    "\n",
    "# Ensure the PM columns are of type float64 after replacing outliers\n",
    "for column in pm_columns:\n",
    "    PM_data_no_outliers[column] = PM_data_no_outliers[column].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fef6589-d8ee-4301-8a51-4d0c17ef4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Drop rows with NaN values \n",
    "PM_data_filtered = PM_data_filtered.dropna()\n",
    "\n",
    "# Set up the figure size for the subplots\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a 2x2 grid of subplots for the PM values\n",
    "# First subplot for PM1\n",
    "plt.subplot(2, 2, 1)  # 2 rows, 2 columns, first subplot\n",
    "sns.boxplot(data=PM_data_filtered, x='device_id', y='Finedust PM1')\n",
    "plt.title('Finedust PM1 Concentration by Device ID')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('Finedust PM1')\n",
    "plt.xticks([])\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Second subplot for PM2.5\n",
    "plt.subplot(2, 2, 2)  # 2 rows, 2 columns, second subplot\n",
    "sns.boxplot(data=PM_data_filtered, x='device_id', y='Finedust PM2.5')\n",
    "plt.title('Finedust PM2.5 Concentration by Device ID')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('Finedust PM2.5')\n",
    "plt.xticks([])\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Third subplot for PM4\n",
    "plt.subplot(2, 2, 3)  # 2 rows, 2 columns, third subplot\n",
    "sns.boxplot(data=PM_data_filtered, x='device_id', y='Finedust PM4')\n",
    "plt.title('Finedust PM4 Concentration by Device ID')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('Finedust PM4')\n",
    "plt.xticks([])\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Fourth subplot for PM10\n",
    "plt.subplot(2, 2, 4)  # 2 rows, 2 columns, fourth subplot\n",
    "sns.boxplot(data=PM_data_filtered, x='device_id', y='Finedust PM10')\n",
    "plt.title('Finedust PM10 Concentration by Device ID')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('Finedust PM10')\n",
    "plt.xticks([])\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Adjust layout to avoid overlap of labels\n",
    "plt.tight_layout()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9b8594-74ca-4eee-ae2a-3bda3a0600e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'PM_data_filtered' has 'lng' and 'lat' columns with the longitude and latitude data\n",
    "pm25_data_heatmap = PM_data_no_outliers[['lat', 'lng', 'Finedust PM2.5']]\n",
    "\n",
    "pm25_data_heatmap = pm25_data_heatmap.dropna(subset=['lat', 'lng', 'Finedust PM2.5'])\n",
    "\n",
    "# Create the map centered around the average lat/lon\n",
    "m_PM25 = folium.Map(location=[51.9607, 7.6261], zoom_start=13)\n",
    "\n",
    "# Prepare the heatmap data: [latitude, longitude, PM2.5 value]\n",
    "heat_data_25 = pm25_data_heatmap[['lat', 'lng', 'Finedust PM2.5']].values\n",
    "\n",
    "# Create and add the heatmap to the map\n",
    "HeatMap(heat_data_25, radius = 15, blur=15).add_to(m_PM25)\n",
    "\n",
    "m_PM25.save(\"PM2.5 heatmap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db63da91-c80e-423e-9143-1a5356b459d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'PM_data_filtered' has 'lat', 'lng', 'Finedust PM2.5', and 'createdAt' columns\n",
    "\n",
    "# Convert the 'createdAt' column to datetime\n",
    "PM_data_filtered['createdAt'] = pd.to_datetime(PM_data_filtered['createdAt'])\n",
    "\n",
    "# Extract the month and time from 'createdAt'\n",
    "PM_data_filtered['month'] = PM_data_filtered['createdAt'].dt.month\n",
    "PM_data_filtered['time_of_day'] = PM_data_filtered['createdAt'].dt.time\n",
    "\n",
    "# Define a function to categorize the months into seasons\n",
    "def get_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "# Apply the function to create a new 'season' column\n",
    "PM_data_filtered['season'] = PM_data_filtered['month'].apply(get_season)\n",
    "\n",
    "# Define the time range filter\n",
    "start_time = pd.to_datetime(\"16:00\", format=\"%H:%M\").time()  # Start time (6:00 AM)\n",
    "end_time = pd.to_datetime(\"18:00\", format=\"%H:%M\").time()  # End time (10:00 AM)\n",
    "\n",
    "# Function to filter data based on both season and time\n",
    "def filter_data(time_filter=True, season_filter=True):\n",
    "    filtered_data = PM_data_filtered\n",
    "    \n",
    "    # Apply the time filter if enabled\n",
    "    if time_filter:\n",
    "        filtered_data = filtered_data[(filtered_data['time_of_day'] >= start_time) & \n",
    "                                      (filtered_data['time_of_day'] <= end_time)]\n",
    "    \n",
    "    # Apply the season filter if enabled\n",
    "    if season_filter:\n",
    "        # Select a season to filter (change as needed or pass as an argument)\n",
    "        selected_season = 'Autumn' \n",
    "        filtered_data = filtered_data[filtered_data['season'] == selected_season]\n",
    "    \n",
    "    return filtered_data[['lat', 'lng', 'Finedust PM2.5']].dropna(subset=['lat', 'lng', 'Finedust PM2.5'])\n",
    "\n",
    "# Now, you can choose to filter by time, season, or both\n",
    "data_filtered = filter_data(time_filter=True, season_filter=True)\n",
    "\n",
    "# Create the map centered around the average lat/lon\n",
    "m_PM25_combined = folium.Map(location=[51.9607, 7.6261], zoom_start=13)\n",
    "\n",
    "# Prepare the heatmap data: [latitude, longitude, PM2.5 value]\n",
    "heat_data_combined = data_filtered[['lat', 'lng', 'Finedust PM2.5']].values\n",
    "\n",
    "# Create and add the heatmap to the map\n",
    "HeatMap(heat_data_combined, radius=15, blur=15).add_to(m_PM25_combined)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m_PM25_combined.save(\"Variable_PM2.5_heatmap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15359975-d367-451a-b530-8d3e5b0efeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'PM_data_filtered' has 'lng' and 'lat' columns with the longitude and latitude data\n",
    "pm10_data_heatmap = PM_data_no_outliers[['lat', 'lng', 'Finedust PM10']]\n",
    "\n",
    "pm10_data_heatmap = pm10_data_heatmap.dropna(subset=['lat', 'lng', 'Finedust PM10'])\n",
    "\n",
    "# Create the map centered around the average lat/lon\n",
    "m_PM10 = folium.Map(location=[51.9607, 7.6261], zoom_start=13)\n",
    "\n",
    "# Prepare the heatmap data: [latitude, longitude, PM10 value]\n",
    "heat_data_10 = pm10_data_heatmap[['lat', 'lng', 'Finedust PM10']].values\n",
    "\n",
    "# Create and add the heatmap to the map\n",
    "HeatMap(heat_data_10, radius = 15, blur = 15).add_to(m_PM10)\n",
    "\n",
    "m_PM10.save(\"PM10 heatmap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd09b5bd-424d-4e1f-8fba-4492cb706312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#from sklearn.cluster import DBSCAN\n",
    "#from geopy.geocoders import Nominatim\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "\n",
    "#PM_city_clusters = PM_data_filtered.copy()\n",
    "#PM_city_clusters = PM_data_no_outliers.copy() #ohne Ausreißer\n",
    "#PM_city_clusters = PM_city_clusters.dropna()\n",
    "\n",
    "#PM_city_clusters['lat'] = PM_city_clusters['lat'].astype('float32')\n",
    "#PM_city_clusters['lng'] = PM_city_clusters['lng'].astype('float32')\n",
    "#coords = np.radians(PM_city_clusters[['lat', 'lng']])\n",
    "\n",
    "# Convert degrees to radians (required for haversine metric)\n",
    "#coords = np.radians(PM_city_clusters[['lat', 'lng']])\n",
    "\n",
    "# Apply DBSCAN clustering with haversine metric\n",
    "#clusterer = DBSCAN(eps=0.001, min_samples=10, metric='haversine')  # Adjust eps as needed # eps=0.001 ~ 6,370 m\n",
    "#PM_city_clusters['cluster'] = clusterer.fit_predict(coords)\n",
    "\n",
    "# Initialize geolocator\n",
    "#geolocator = Nominatim(user_agent=\"sensebox bike\")\n",
    "\n",
    "# Function to get city name\n",
    "#def get_city(lat, lon):\n",
    "#    time.sleep(1)\n",
    "#    location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "#    return location.raw['address'].get('city', 'Unknown')\n",
    "\n",
    "# Reverse geocode one point per cluster\n",
    "#cluster_centroids = PM_city_clusters.groupby('cluster')[['lat', 'lng']].first().reset_index()\n",
    "#cluster_centroids['city'] = cluster_centroids.apply(lambda row: get_city(row['lat'], row['lng']), axis=1)\n",
    "\n",
    "# Map the city names back to the original DataFrame\n",
    "#PM_cities = PM_city_clusters.merge(cluster_centroids[['cluster', 'city']], on='cluster', how='left')\n",
    "#PM_cities['city'] = PM_cities['city'].replace('São Paulo', 'Sao Paulo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56679bde-ce4f-4ace-8cf2-02f45b7295c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Münster's bounding box (example values)\n",
    "min_lat, max_lat = 51.90, 52.10  # Approximate latitude range\n",
    "min_lon, max_lon = 7.50, 7.75    # Approximate longitude range\n",
    "\n",
    "# Filter data within the bounding box\n",
    "muenster_data = PM_data_filtered[\n",
    "    (PM_data_filtered['lat'] >= min_lat) & (PM_data_filtered['lat'] <= max_lat) &\n",
    "    (PM_data_filtered['lng'] >= min_lon) & (PM_data_filtered['lng'] <= max_lon)\n",
    "]\n",
    "\n",
    "# Set up the figure size for the subplots\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a 2x2 grid of subplots for the PM values\n",
    "# First subplot for PM1\n",
    "plt.subplot(2, 2, 1)  # 2 rows, 2 columns, first subplot\n",
    "sns.boxplot(data=muenster_data, x='device_id', y='Finedust PM1')\n",
    "plt.title('PM1 Concentration by Device ID, Münster')\n",
    "plt.xlabel('Devices')\n",
    "plt.ylabel('PM1')\n",
    "plt.xticks([])\n",
    "plt.ylim(0,80)\n",
    "\n",
    "# Second subplot for PM2.5\n",
    "plt.subplot(2, 2, 2)  # 2 rows, 2 columns, second subplot\n",
    "sns.boxplot(data=muenster_data, x='device_id', y='Finedust PM2.5')\n",
    "plt.title('PM2.5 Concentration by Device ID, Münster')\n",
    "plt.xlabel('Devices')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.xticks([])\n",
    "plt.ylim(0,80)\n",
    "\n",
    "# Third subplot for PM4\n",
    "plt.subplot(2, 2, 3)  # 2 rows, 2 columns, third subplot\n",
    "sns.boxplot(data=muenster_data, x='device_id', y='Finedust PM4')\n",
    "plt.title('PM4 Concentration by Device ID, Münster')\n",
    "plt.xlabel('Devices')\n",
    "plt.ylabel('PM4')\n",
    "plt.xticks([])\n",
    "plt.ylim(0,80)\n",
    "\n",
    "# Fourth subplot for PM10\n",
    "plt.subplot(2, 2, 4)  # 2 rows, 2 columns, fourth subplot\n",
    "sns.boxplot(data=muenster_data, x='device_id', y='Finedust PM10')\n",
    "plt.title('PM10 Concentration by Device ID, Münster')\n",
    "plt.xlabel('Devices')\n",
    "plt.ylabel('PM10')\n",
    "plt.xticks([])\n",
    "plt.ylim(0,80)\n",
    "\n",
    "# Adjust layout to avoid overlap of labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('PM_Münster.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5453e69f-171f-4bd7-9f68-d3f248368f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data: filter out rows with missing coordinates or PM2.5 values\n",
    "heatmap_data_muenster_PM25 = muenster_data[['lat', 'lng', 'Finedust PM2.5']].dropna()\n",
    "\n",
    "# Calculate the 95th percentile of the Finedust PM2.5 data\n",
    "percentile_95 = np.percentile(heatmap_data_muenster_PM25['Finedust PM2.5'], 95)\n",
    "\n",
    "# Normalize the data by dividing by the 95th percentile\n",
    "heatmap_data_muenster_PM25['normalized_PM2.5'] = (\n",
    "    heatmap_data_muenster_PM25['Finedust PM2.5'] / percentile_95\n",
    ").clip(upper=1)  # Clip values greater than 1 to 1\n",
    "\n",
    "# Create a Folium map centered around Münster\n",
    "m_muenster_PM25 = folium.Map(\n",
    "    location=[51.9607, 7.6261], \n",
    "    zoom_start=13\n",
    ")\n",
    "\n",
    "# Add the heatmap layer using the normalized PM2.5 values\n",
    "HeatMap(\n",
    "    data=heatmap_data_muenster_PM25[['lat', 'lng', 'normalized_PM2.5']].values.tolist(),\n",
    "    radius=15,  # Adjust the radius for the desired heatmap spread\n",
    "    blur=15,\n",
    "    max_zoom=15\n",
    ").add_to(m_muenster_PM25)\n",
    "\n",
    "m_muenster_PM25.save(\"Münster_PM25_hm.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34236678-ad88-45e5-a3d6-7e8085f63254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "muenster_data_diurnal = muenster_data.copy()\n",
    "\n",
    "# Ensure 'createdAt' is in datetime format\n",
    "muenster_data_diurnal['createdAt'] = pd.to_datetime(muenster_data_diurnal['createdAt'])\n",
    "\n",
    "# Step 1: Extract the time (ignoring the date) and round it to 30-minute intervals\n",
    "muenster_data_diurnal['time_30min'] = muenster_data_diurnal['createdAt'].dt.strftime('%H:%M')\n",
    "muenster_data_diurnal['time_30min'] = pd.to_datetime(muenster_data_diurnal['time_30min'], format='%H:%M')\n",
    "\n",
    "# Step 2: Round the time to the nearest 30 minutes\n",
    "muenster_data_diurnal['time_30min'] = muenster_data_diurnal['time_30min'].dt.round('30min')\n",
    "\n",
    "# Step 3: Group by the rounded time and calculate the mean PM2.5 concentration\n",
    "diurnal_cycle = muenster_data_diurnal.groupby('time_30min')['Finedust PM2.5'].mean()\n",
    "\n",
    "# Step 4: Create a full range of 30-minute intervals from 00:00 to 23:30\n",
    "start_time = pd.to_datetime('00:00', format='%H:%M')\n",
    "end_time = pd.to_datetime('23:30', format='%H:%M')\n",
    "date_range = pd.date_range(start=start_time, end=end_time, freq='30min')\n",
    "\n",
    "# Step 5: Reindex the diurnal_cycle to include all the 30-minute intervals and fill missing values with NaN\n",
    "diurnal_cycle_full = diurnal_cycle.reindex(date_range, fill_value=None)\n",
    "\n",
    "# Plot the diurnal cycle as a line graph with markers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(diurnal_cycle_full.index, diurnal_cycle_full.values, marker='o', linestyle='-', color='skyblue')\n",
    "\n",
    "# Format the plot\n",
    "plt.title(\"Diurnal Cycle of PM2.5 Concentrations, Münster\")\n",
    "plt.xlabel(\"Time of Day\")\n",
    "plt.ylabel(\"Average PM2.5 Concentration\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(ticks=diurnal_cycle_full.index, labels=diurnal_cycle_full.index.strftime('%H:%M'), rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('Diurnal_PM2.5_Münster.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f12b69a-6292-484e-8433-c670736ede4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a copy of the data\n",
    "muenster_data_monthly = muenster_data.copy()\n",
    "\n",
    "# Ensure 'createdAt' is in datetime format\n",
    "muenster_data_monthly['createdAt'] = pd.to_datetime(muenster_data_monthly['createdAt'])\n",
    "\n",
    "# Extract the month and year for grouping\n",
    "muenster_data_monthly['month'] = muenster_data_monthly['createdAt'].dt.to_period('M')\n",
    "\n",
    "# Group by month and calculate the mean PM2.5 concentration\n",
    "muenster_monthly_averages = muenster_data_monthly.groupby('month')['Finedust PM2.5'].mean()\n",
    "\n",
    "# Convert the PeriodIndex to a DatetimeIndex for plotting\n",
    "muenster_monthly_averages.index = muenster_monthly_averages.index.to_timestamp()\n",
    "\n",
    "# Plot the monthly averages as a line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(muenster_monthly_averages.index, muenster_monthly_averages.values, marker='o', linestyle='-', color='skyblue')\n",
    "\n",
    "# Format the plot\n",
    "plt.title(\"Monthly Average PM2.5 Concentrations, Münster\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average PM2.5 Concentration\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "#plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('Monthly_PM2.5_Münster.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "418a5acd-6229-4814-83a6-11f703a25987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define São Paulo's bounding box\n",
    "min_lat, max_lat = -24.0, -23.3\n",
    "min_lon, max_lon = -46.825, -46.365\n",
    "\n",
    "# Filter the dataset\n",
    "sao_paulo_data = PM_data_filtered[\n",
    "    (PM_data_filtered['lat'] >= min_lat) & (PM_data_filtered['lat'] <= max_lat) &\n",
    "    (PM_data_filtered['lng'] >= min_lon) & (PM_data_filtered['lng'] <= max_lon)\n",
    "]\n",
    "\n",
    "# Set up the figure size for the subplots\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a 2x2 grid of subplots for the PM values\n",
    "# First subplot for PM1\n",
    "plt.subplot(2, 2, 1)  # 2 rows, 2 columns, first subplot\n",
    "sns.boxplot(data=sao_paulo_data, x='device_id', y='Finedust PM1')\n",
    "plt.title('PM1 Concentration by Device ID, Sao Paulo')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('PM1')\n",
    "plt.xticks(rotation=90)\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Second subplot for PM2.5\n",
    "plt.subplot(2, 2, 2)  # 2 rows, 2 columns, second subplot\n",
    "sns.boxplot(data=sao_paulo_data, x='device_id', y='Finedust PM2.5')\n",
    "plt.title('PM2.5 Concentration by Device ID, Sao Paulo')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.xticks(rotation=90)\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Third subplot for PM4\n",
    "plt.subplot(2, 2, 3)  # 2 rows, 2 columns, third subplot\n",
    "sns.boxplot(data=sao_paulo_data, x='device_id', y='Finedust PM4')\n",
    "plt.title('PM4 Concentration by Device ID, Sao Paulo')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('PM4')\n",
    "plt.xticks(rotation=90)\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Fourth subplot for PM10\n",
    "plt.subplot(2, 2, 4)  # 2 rows, 2 columns, fourth subplot\n",
    "sns.boxplot(data=sao_paulo_data, x='device_id', y='Finedust PM10')\n",
    "plt.title('PM10 Concentration by Device ID, Sao Paulo')\n",
    "plt.xlabel('Device ID')\n",
    "plt.ylabel('PM10')\n",
    "plt.xticks(rotation=90)\n",
    "#plt.ylim(0,80)\n",
    "\n",
    "# Adjust layout to avoid overlap of labels\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('PM_Sao_Paulo.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d66fce-a9df-4184-9f64-5c4e69ec6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sao_paulo_data_diurnal = sao_paulo_data.copy()\n",
    "\n",
    "sao_paulo_data_diurnal = sao_paulo_data_diurnal[sao_paulo_data_diurnal['device_id'] != \"Giro+bikeAtrai\"]\n",
    "\n",
    "# Ensure 'createdAt' is in datetime format\n",
    "sao_paulo_data_diurnal['createdAt'] = pd.to_datetime(sao_paulo_data_diurnal['createdAt'])\n",
    "\n",
    "# Step 1: Extract the time (ignoring the date) and round it to 30-minute intervals\n",
    "sao_paulo_data_diurnal['time_30min'] = sao_paulo_data_diurnal['createdAt'].dt.strftime('%H:%M')\n",
    "sao_paulo_data_diurnal['time_30min'] = pd.to_datetime(sao_paulo_data_diurnal['time_30min'], format='%H:%M')\n",
    "\n",
    "# Step 2: Round the time to the nearest 30 minutes\n",
    "sao_paulo_data_diurnal['time_30min'] = sao_paulo_data_diurnal['time_30min'].dt.round('30min')\n",
    "\n",
    "# Step 3: Group by the rounded time and calculate the mean PM2.5 concentration\n",
    "diurnal_cycle_sp = sao_paulo_data_diurnal.groupby('time_30min')['Finedust PM2.5'].mean()\n",
    "\n",
    "# Step 4: Create a full range of 30-minute intervals from 00:00 to 23:30\n",
    "start_time = pd.to_datetime('00:00', format='%H:%M')\n",
    "end_time = pd.to_datetime('23:30', format='%H:%M')\n",
    "date_range = pd.date_range(start=start_time, end=end_time, freq='30min')\n",
    "\n",
    "# Step 5: Reindex the diurnal_cycle to include all the 30-minute intervals and fill missing values with NaN\n",
    "diurnal_cycle_full_sp = diurnal_cycle_sp.reindex(date_range, fill_value=None)\n",
    "\n",
    "# Plot the diurnal cycle as a line graph with markers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(diurnal_cycle_full_sp.index, diurnal_cycle_full_sp.values, marker='o', linestyle='-', color='skyblue')\n",
    "\n",
    "# Format the plot\n",
    "plt.title(\"Diurnal Cycle of PM2.5 Concentrations, Sao Paulo\")\n",
    "plt.xlabel(\"Time of Day\")\n",
    "plt.ylabel(\"Average PM2.5 Concentration\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(ticks=diurnal_cycle_full_sp.index, labels=diurnal_cycle_full_sp.index.strftime('%H:%M'), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Diurnal_PM2.5_Sao_Paolo.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ae7bb6-e99a-474e-8c3f-e33271cbad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_station = pd.read_csv('Daten_Weseler_Strasse_Verkehr.csv')\n",
    "background_station = pd.read_csv('Daten_Geist_Background.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d072e66e-0a00-4d69-a58a-5f3fdbc2cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'datum_beginn' is in datetime format\n",
    "background_station['datum_beginn'] = pd.to_datetime(background_station['datum_beginn'])\n",
    "traffic_station['datum_beginn'] = pd.to_datetime(traffic_station['datum_beginn'])\n",
    "\n",
    "# Create the 'month' column by extracting the year and month\n",
    "background_station['month'] = background_station['datum_beginn'].dt.strftime('%Y-%m')\n",
    "traffic_station['month'] = traffic_station['datum_beginn'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Convert 'MSGE LUFT ONLINE' in background_station to float\n",
    "background_station['MSGE LUFT ONLINE'] = pd.to_numeric(background_station['MSGE LUFT ONLINE'], errors='coerce')\n",
    "\n",
    "# Convert 'VMS2 LUFT ONLINE' in traffic_station to float\n",
    "traffic_station['VMS2 LUFT ONLINE'] = pd.to_numeric(traffic_station['VMS2 LUFT ONLINE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ff09631-1e2d-4195-9218-d17e666e93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 'month' is the index, reset it to a column\n",
    "muenster_comparison = muenster_monthly_averages.reset_index()\n",
    "muenster_comparison['Finedust PM2.5'] = muenster_comparison['Finedust PM2.5'].round()\n",
    "muenster_comparison = muenster_comparison.rename(columns={'Finedust PM2.5': 'monthly averages'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c76c65f-9bba-4c2a-b93f-1f916ce0bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'month' columns to datetime format for all DataFrames\n",
    "background_station['month'] = pd.to_datetime(background_station['month'])\n",
    "traffic_station['month'] = pd.to_datetime(traffic_station['month'])\n",
    "muenster_comparison['month'] = pd.to_datetime(muenster_comparison['month'])\n",
    "\n",
    "# Find common months across all three datasets\n",
    "common_months_all = (\n",
    "    set(background_station['month'])\n",
    "    .intersection(set(traffic_station['month']))\n",
    "    .intersection(set(muenster_comparison['month']))\n",
    ")\n",
    "\n",
    "# Filter each DataFrame to include only rows with common months\n",
    "background_station_filtered = background_station[background_station['month'].isin(common_months_all)]\n",
    "traffic_station_filtered = traffic_station[traffic_station['month'].isin(common_months_all)]\n",
    "muenster_monthly_averages_filtered = muenster_comparison[muenster_comparison['month'].isin(common_months_all)]\n",
    "\n",
    "# Sort the dataframes by the month for consistent x-axis order\n",
    "background_station_filtered = background_station_filtered.sort_values(by='month')\n",
    "traffic_station_filtered = traffic_station_filtered.sort_values(by='month')\n",
    "muenster_monthly_averages_filtered = muenster_monthly_averages_filtered.sort_values(by='month')\n",
    "\n",
    "# Plot all three datasets\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(\n",
    "    background_station_filtered['month'], \n",
    "    background_station_filtered['MSGE LUFT ONLINE'], \n",
    "    marker='o', label='Background Station', linestyle='-'\n",
    ")\n",
    "plt.plot(\n",
    "    traffic_station_filtered['month'], \n",
    "    traffic_station_filtered['VMS2 LUFT ONLINE'], \n",
    "    marker='x', label='Traffic Station', linestyle='--'\n",
    ")\n",
    "plt.plot(\n",
    "    muenster_monthly_averages_filtered['month'], \n",
    "    muenster_monthly_averages_filtered['monthly averages'], \n",
    "    marker='s', label='senseBox:bike data', linestyle='-.'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Year and Month')\n",
    "plt.ylabel('Average Monthly PM2.5 Concentrations (µg/m³)')\n",
    "plt.title('Comparing PM2.5 in Münster')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Comparison PM2.5 in Münster.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
